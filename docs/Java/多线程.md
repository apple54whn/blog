# 多线程

> Java 经过这些年的发展，Java SDK 并发包提供了非常丰富的功能，对于初学者来说可谓是眼花缭乱，好多人觉得无从下手。但是，Java SDK 并发包乃是并发大师 Doug Lea 出品，堪称经典，它内部一定是有章可循的。那它的章法在哪里呢？
>
> Java 只是根据自身情况对并发编程做了实现

## 引入

### 分工

**分工**：指的是如何高效地**拆解任务并分配给线程**。

* Java SDK 并发包里的 Executor、Fork/Join、 Future 本质上都是一种分工方法。并发编程领域还总结了一些设计模式，基本上都是和分工方法相关的，例如**生产者 - 消费者**、Thread-Per-Message、Worker Thread 模式等都是用来指导你如何分工的。

### 同步

**同步**：指的是**线程之间**如何**协作**。一个线程执行完了一个任务，如何通知执行后续任务的线程开工。

* **协作**一般是**和分工相关**的。Java SDK 并发包里的 **Executor、Fork/Join、Future** 本质上都是分工方法，但同时也能解决线程协作的问题。如，用 Future 可以发起一个异步调用，当主线程通过 get() 方法取结果时，主线程就会等待，当异步执行的结果返回时， get() 方法就自动返回了。主线程和异步线程之间的协作，Future 工具类已经帮我们解决了。除此之外，Java SDK 里提供的 CountDownLatch、CyclicBarrier、Phaser、 Exchanger 也都是用来解决线程协作问题的。
* 工作中遇到的线程协作问题，基本上可描述：当某个条件不满足时线程需要等待，当某个条件满足时线程需要被唤醒执行。

### 互斥

**分工、同步**主要强调的是**性能**。但并发程序里还有一部分是关于**正确性**的，用专业术语叫“**线程安全**”

当多个线程同时访问同一个共享变量的时候，结果是不确定的。其主要源头是**可见性**问题、**有序性**问题和**原子性**问题

为了解决这三个问题，Java 语言引入了**内存模型**，内存模型提供了一系列的规则，利用这些规则，我们可以避免可见性问题、有序性问题，但是还不足以完全解决线程安全问题。

解决线程安全问题的核心方案还是互斥！**实现互斥的核心技术就是锁**，如synchronized，Lock等

虽说锁解决了安全性问题，但同时也带来了**性能问题**，那如何保证安全性的同时又尽量提高性能呢？可以分场景优化，Java SDK 里提供的 ReadWriteLock、 StampedLock 就可以优化读多写少场景下锁的性能。还可以使用无锁的数据结构，例如Java SDK 里提供的原子类都是基于无锁技术实现的。除此之外，还有一些其他的方案，原理是**不共享变量或者变量只允许读**。这方面，Java 提供了 ThreadLocal 和 final 关键字，还有一种 Copy-on-write 的模式。

使用锁除了要注意性能问题外，还需要注意**死锁问题**。这部分内容比较复杂，往往还是跨领域的。要理解可见性，就需要了解一些 CPU 和缓存的知识；要理解原子性，就需要理解一些操作系统的知识；很多无锁算法的实现往往也需要理解 CPU 缓存

**互斥**：则是保证**同一时刻只允许一个线程访问共享资源**。

* 如**可重入锁**则是一种互斥手段



### 并发容器



### 原子类



### 并发编程图

![1554089620212](/images/Java/1554089620212.png)




### 管程模型

例如，Java 里 synchronized、wait()、notify() 不过是**操作系统**领域里**管程模型的一种实现**而已，Java SDK 并发包里的条件变量 **Condition 也是管程里的概念**，synchronized、wait()、notify()、条件变量这些知识如果单独理解，自然是管中窥豹。但是如果站在管程这个理论模型的高度，你就会发现这些知识原来这么简单，同时用起来也就得心应手了。

**管程**作为一种**解决并发问题的模型**，是继**信号量模型**之后的一项重大创新，它与信号量在逻辑上是等价的（可以用管程实现信号量，也可以用信号量实现管程），但是相比之下管程**更易用**。而且，很多编程语言都支持管程，搞懂管程，对学习其他很多语言的并发编程有很大帮助。然而，很多人急于学习 Java 并发编程技术，却忽略了技术背后的理论和模型，而理论和模型却往往比具体的技术更为重要。

管程是一种解决并发问题的通用模型，**管程是解决并发问题的万能钥匙**



## 可见性、原子性、有序性问题

> 并发编程Bug的源头就是如上三个问题。

### 并发程序幕后的故事

这些年，我们的 **CPU、内存、I/O** 设备都在不断迭代，不断朝着更快的方向努力。但是，在这个快速发展的过程中，有一个核心矛盾一直存在，就是这三者的速度差异。为了合理利用 CPU 的高性能，平衡这三者的速度差异，计算机体系机构、操作系统、编译程序都做出了贡献，主要体现为：

* **CPU** 增加了**缓存**，以均衡与内存的速度差异
* 操作系统增加了**进程、线程**，以**分时复用 CPU**，进而均衡 CPU 与 I/O 设备的速度差异
* 编译程序**优化指令执行次序**，使得**缓存**能够得到更加**合理地利用**

但是并发程序很多诡异问题的根源也在这里。



### 缓存导致的可见性问题

在单核时代，所有的线程都是在一颗 CPU 上执行，CPU 缓存与内存的数据一致性容易解决。因为所有线程都是操作同一个 CPU 的缓存，一个线程对缓存的写，对另外一个线程来说一定是可见的。例如在下面的图中，线程 A 和线程 B 都是操作同一个 CPU 里面的缓存，所以线程 A 更新了变量 V 的值，那么线程 B 之后再访问变量 V，得到的一定是 V 的最新值（线程 A 写过的值）。

![1554099709867](/images/Java/1554099709867.png)

**一个线程对共享变量的修改，另外一个线程能够立刻看到**，我们称为**可见性**。

多核时代，每颗 CPU 都有自己的缓存，这时 CPU 缓存与内存的数据一致性就没那么容易解决了，当多个线程在不同的 CPU 上执行时，这些线程操作的是不同的 CPU 缓存。比如下图中，线程 A 操作的是 CPU-1 上的缓存，而线程 B 操作的是 CPU-2 上的缓存，很明显，这个时候线程 A 对变量 V 的操作对于线程 B 而言就不具备可见性了。这个就属于硬件程序员给软件程序员挖的“坑”。

![1554099859573](/images/Java/1554099859573.png)

下面我们再用一段代码来验证一下多核场景下的可见性问题。下面的代码，每执行一次add10K() 方法，都会循环 10000 次 count+=1 操作。在 calc() 方法中我们创建了两个线程，每个线程调用一次 add10K() 方法，我们来想一想执行 calc() 方法得到的结果应该是多少呢？

```java
public class Test {
    private long count = 0;
    private void add10K() {
        int idx = 0;
        while(idx++ < 10000) {
            count += 1;
        }
    }
    public static long calc() throws InterruptedException {
        final Test test = new Test();
        // 创建两个线程，执行 add() 操作
        Thread th1 = new Thread(test::add10K);
        Thread th2 = new Thread(test::add10K);

        // 启动两个线程
        th1.start();
        th2.start();
        // 等待两个线程执行结束
        th1.join();
        th2.join();
        return test.count;
    }
}
```

直觉告诉我们应该是 20000，但实际上 calc() 的执行结果是个 10000 到 20000 之间的随机数。为什么呢？

我们假设线程 A 和线程 B **同时开始执行**，那么**第一次都会将 count=0 读到各自的 CPU 缓存里**，执行完 count+=1 之后，各自 CPU 缓存里的值都是 1，**同时写入内存**后，我们会发现内存中是 1，而不是我们期望的 2。之后由于各自的 CPU 缓存里都有了 count 的值，**两个线程都是基于 CPU 缓存里的 count 值来计算**，所以导致最终 count 的值都是小于 20000 的。这就是缓存的可见性问题。

循环 10000 次 count+=1 操作如果改为循环 1 亿次，你会发现效果更明显，最终 count 的值接近 1 亿，而不是 2 亿。如果循环 10000 次，count 的值接近 20000，原因是两个线程不是同时启动的，有一个时差。（10000的俩线程可能其中一个都快执行完了，另一个才开启，所以更靠近20000；1亿的俩线程，一个执行了快10000条，另一个启动了，所以更靠近1亿）



### 线程切换带来的原子性问题

由于 IO 太慢，早期的操作系统就发明了多进程，即便在单核的 CPU 上我们也可以一边听着歌，一边写 Bug，这个就是多进程的功劳。操作系统允许某个进程执行一小段时间，例如 50 毫秒，过了 50 毫秒操作系统就会重新选择一个进程来执行（我们称为“任务切换”），这个 50 毫秒称为“时间片”。

![1554101594247](/images/Java/1554101594247.png)

在一个时间片内，如果一个进程进行一个 IO 操作，例如读个文件，这个时候该进程可以把自己标记为“休眠状态”并出让 CPU 的使用权，待文件读进内存，操作系统会把这个休眠的进程唤醒，唤醒后的进程就有机会重新获得 CPU 的使用权了。

这里的进程在等待 IO 时之所以会释放 CPU 使用权，是为了让 CPU 在这段等待时间里可以做别的事情，这样一来 CPU 的使用率就上来了；此外，如果这时有另外一个进程也读文件，读文件的操作就会排队，磁盘驱动在完成一个进程的读操作后，发现有排队的任务，就会立即启动下一个读操作，这样 IO 的使用率也上来了。

虽然看似简单，但支持多进程分时复用在操作系统的发展史上却具有里程碑意义，Unix 就是因为解决了这个问题而名噪天下的。

早期的操作系统基于进程来调度 CPU，不同进程间是不共享内存空间的，所以进程要做任务切换就要切换内存映射地址，而一个进程创建的所有线程，都是共享一个内存空间的，所以线程做任务切换成本就很低了。现代的操作系统都基于更轻量的线程来调度，现在我们提到的“任务切换”都是指“线程切换”。

Java 并发程序都是基于多线程的，自然也会涉及到任务切换，也许你想不到，任务切换竟然也是并发编程里诡异 Bug 的源头之一。任务切换的时机大多数是在时间片结束的时候，我们现在基本都使用高级语言编程，高级语言里一条语句往往需要多条 CPU 指令完成，例如上面代码中的count += 1，至少需要三条 CPU 指令。

```
指令 1：首先，需要把变量 count 从内存加载到 CPU 的寄存器；
指令 2：之后，在寄存器中执行 +1 操作；
指令 3：最后，将结果写入内存（缓存机制导致可能写入的是 CPU 缓存而不是内存）;
```

**操作系统做任务切换，可以发生在任何一条CPU 指令执行完**，是的，**是 CPU 指令**，而不是高级语言里的一条语句。对于上面的三条指令来说，我们假设 count=0，如果线程 A在指令 1 执行完后做线程切换，线程 A 和线程 B 按照下图的序列执行，那么我们会发现两个线程都执行了 count+=1 的操作，但是得到的结果不是我们期望的 2，而是 1。

![1554102014314](/images/Java/1554102014314.png)

我们潜意识里面觉得 count+=1 这个操作是一个不可分割的整体，就像一个原子一样，线程的切换可以发生在 count+=1 之前，也可以发生在 count+=1 之后，但就是不会发生在中间。

我们把**一个或者多个操作在 CPU 执行的过程中不被中断的特性称为原子性**。

CPU 能保证的原子操作是 CPU 指令级别的，而不是高级语言的操作符，这是违背我们直觉的地方。因此，很多时候我们需要在高级语言层面保证操作的原子性。



### 编译优化带来的有序性问题

那并发编程里还有没有其他有违直觉容易导致诡异 Bug 的技术呢？有的，就是有序性。顾名思义，有序性指的是程序按照代码的先后顺序执行。编译器为了优化性能，有时候会改变程序中语句的先后顺序，例如程序中：“a=6；b=7；”编译器优化后可能变
成“b=7；a=6；”，在这个例子中，编译器调整了语句的顺序，但是不影响程序的最终结果。不过有时候编译器及解释器的优化可能导致意想不到的 Bug。

在 Java 领域一个经典的案例就是利用**双重检查创建单例对象**，例如下面的代码：在获取实例 getInstance() 的方法中，我们首先判断 instance 是否为空，如果为空，则锁定Singleton.class 并再次检查 instance 是否为空，如果还为空则创建 Singleton 的一个实例。

```java
public class Singleton {
    static Singleton instance;  
    static Singleton getInstance() {
        if (instance == null) {
            synchronized(Singleton.class) {
                if (instance == null)
                    instance = new Singleton();  }
        }
        return instance;
    }
}
```

假设有两个线程 A、B 同时调用 getInstance() 方法，他们会同时发现 instance == null ，于是同时对 Singleton.class 加锁，此时 JVM 保证只有一个线程能够加锁成功（假设是线程 A），另外一个线程则会处于等待状态（假设是线程 B）；线程 A 会创建一
个 Singleton 实例，之后释放锁，锁释放后，线程 B 被唤醒，线程 B 再次尝试加锁，此时是可以加锁成功的，加锁成功后，线程 B 检查 instance == null 时会发现，已经创建过 Singleton 实例了，所以线程 B 不会再创建一个 Singleton 实例。

这看上去一切都很完美，无懈可击，但实际上这个 getInstance() 方法并不完美。问题出在哪里呢？出在 new 操作上，我们以为的 new 操作应该是：

```
1.	分配一块内存 M；
2.	在内存 M 上初始化 Singleton 对象；
3.	然后 M 的地址赋值给 instance 变量。
```

但是实际上优化后的执行路径却是这样的（**指令重排序**）：

```
1.	分配一块内存 M；
2.	将 M 的地址赋值给 instance 变量；
3.	最后在内存 M 上初始化 Singleton 对象。
```

优化后会导致什么问题呢？我们假设线程 A 先执行 getInstance() 方法，当执行完指令2时恰好发生了线程切换，切换到了线程 B 上；如果此时线程 B 也执行 getInstance() 方法，那么线程 B 会发现instance != null，所以直接返回 instance，而此时的instance 是没有初始化过的，如果我们这个时候访问 instance 的成员变量就可能触发空指针异常。

![1554102446306](/images/Java/1554102446306.png)



在介绍可见性、原子性、有序性的时候，特意提到**缓存**导致的可见性问题，**线程切换**带来的原子性问题，**编译优化**带来的有序性问题，其实缓存、线程、编译优化的目的和我们写并发程序的目的是相同的，都是提高程序性能。但是技术在解决一个问题的同时，必然会带来另外一个问题，所以在采用一项技术的同时，一定要清楚它带来的问题是什么，以及如何规避。



## Java内存模型—解决可见性、有序性

### Java 内存模型？

你已经知道，导致**可见性**的原因是**缓存**，导致**有序性**的原因是**编译优化**（指令重排），那解决可见性、有序性最直接的办法就是禁用缓存和编译优化，但是这样问题虽然解决了，我们程序的性能可就堪忧了。合理的方案应该是**按需禁用**缓存以及编译优化。

Java 内存模型**规范**了 JVM 如何提供按需禁用缓存和编译优化的方法。具体来说，这些方法包括 **volatile、synchronized 和 final** 三个关键字，以及**六项 Happens-Before 规则**，这也正是本期的重点内容。



### 使用 volatile 的困惑

>  ['vɒlətaɪl] adj. 易变的；无定性的

C 语言里也有，它最原始的意义就是**禁用 CPU 缓存**。

例如，我们声明一个 volatile 变量 volatile int x = 0，它表达的是：告诉编译器，对这个变量的读写，不能使用 CPU 缓存，必须从内存中读取或者写入。这个语义看上去相当明确，但是在实际使用的时候却会带来困惑。

例如下面的示例代码，假设线程 A 执行 writer() 方法，按照 volatile 语义，会把变量“v=true” 写入内存；假设线程 B 执行 reader() 方法，同样按照 volatile 语义，线程 B会从内存中读取变量 v，如果线程 B 看到 “v == true” 时，那么线程 B 看到的变量 x 是多少呢？

直觉上看，应该是 42，那实际应该是多少呢？这个要看 Java 的版本，如果在低于 1.5 版本上运行，x 可能是 42，也有可能是 0；如果在 1.5 以上的版本上运行，x 就是等于 42。

```java
// 以下代码来源于【参考 1】
class VolatileExample {
    int x = 0;
    volatile boolean v = false;
    public void writer() {
        x = 42;
        v = true;
    }
    public void reader() {
        if (v == true) {
            // 这里 x 会是多少呢？
        }
    }
}
```

分析一下，为什么 1.5 以前的版本会出现 x = 0 的情况呢？我相信你一定想到了，**变量 x可能被 CPU 缓存**而导致可见性问题。这个问题在 1.5 版本已经被圆满解决了。**Java 内存模型在 1.5 版本对 volatile 语义进行了增强**。怎么增强的呢？答案是一项 Happens-Before 规则。

### Happens-Before 规则

Happens-Before 并不是说前面一个操作发生在后续操作的前面，它真正要表达的是：**前面一个操作的结果对后续操作是可见的**。

正式的说法是：Happens-Before 约束了编译器的优化行为，虽允许编译器优化，但是要求编译器优化后一定遵守 Happens-Before 规则。

Happens-Before 规则应该是 Java 内存模型里面最晦涩的内容了，和程序员相关的规则一共有如下六项，都是关于可见性的。前面示例代码涉及到这六项规则中的前三项，来看规则 1、2 和 3 到底该如何理解。

* **程序的顺序性规则**

  这条规则是指在**一个线程中**，按照程序顺序，**前面的操作** Happens-Before 于**后续的任意操作**。这还是比较容易理解的，比如刚才那段示例代码，按照程序的顺序，第 6 行代码 “x = 42;” Happens-Before 于第 7 行代码 “v = true;”，这就是规则 1 的内容，也比较符合单线程里面的思维：程序前面对某个变量的修改一定是对后续操作可见的。

* **volatile 变量规则**

  这条规则是指**对一个 volatile 变量的写操作**， Happens-Before 于后续**对这个 volatile变量的读操作**。这个就有点费解了，对一个 volatile 变量的写操作相对于后续对这个 volatile 变量的读操作可见，这怎么看都是禁用缓存的意思啊，貌似和 1.5 版本以前的语义没有变化啊？如果单看这个规则，的确是这样，但是如果我们关联一下规则 3，就有点不一样的感觉了。

* **传递性**

  这条规则是指如果 A Happens-Before B，且 B Happens-Before C，那么 A Happens-Before C。我们将规则 3 的传递性应用到我们的例子中，会发生什么呢？可以看下面这幅图：

  ![1554106667071](images/1554106667071.png)

  从图中，我们可以看到：

  1.	“x=42” Happens-Before 写变量 “v=true” ，这是规则 1 的内容；
  2.	写变量“v=true” Happens-Before 读变量 “v=true”，这是规则 2 的内容 。
  3.	再根据这个传递性规则，我们得到结果：“x=42” Happens-Before 读变量“v=true”。这意味着什么呢？

  如果线程 B 读到了“v=true”，那么线程 A 设置的“x=42”对线程 B 是可见的。也就是说，线程 B 能看到 “x == 42” ，有没有一种恍然大悟的感觉？这就是 1.5 版本对volatile 语义的增强，这个增强意义重大，1.5 版本的并发工具（java.util.concurrent）就是靠 volatile 语义来搞定可见性的，这个在后面的内容中会详细介绍。

* **管程中锁的规则**

  这条规则是指对**一个锁的解锁** Happens-Before 于后续**对这个锁的加锁**。

  要理解这个规则，就首先要了解“管程指的是什么”。管程是一种通用的同步原语，在Java 中指的就是 synchronized，**synchronized 是 Java 里对管程的实现**。**管程中的锁在 Java 里是隐式实现的**，例如下面的代码，在**进入同步块之前，会自动加锁，而在代码块执行完会自动释放锁**，加锁以及释放锁都是**编译器帮我们实现**的。

  ```java
  synchronized (this) { // 此处自动加锁
      // x 是共享变量, 初始值 =10
      if (this.x < 12) {
          this.x = 12; 
      }  
  } // 此处自动解锁
  ```

  所以结合规则 4——管程中锁的规则，可以这样理解：假设 x 的初始值是 10，线程 A 执行完代码块后 x 的值会变成 12（执行完自动释放锁），线程 B 进入代码块时，能够看到线程 A 对 x 的写操作，也就是线程 B 能够看到 x==12。这个也是符合我们直觉的，应该不难理解。

* **线程 start() 规则**

  这条是关于线程启动的。它是指**主线程 A 启动子线程 B 后**，子线程 B 能够**看到**主线程在**启动子线程 B 前的操作**。当然所谓的“看到”，指的是对**共享变量的操作**。

  换句话说就是，如果线程 A 中调用线程 B 的 start() 方法（即在线程 A 中启动线程 B），那么该 start() 操作 Happens-Before 于线程 B 中的任意操作。具体可参考下面示例代码。

  ```java
  Thread B = new Thread(()->{
      // 主线程调用 B.start() 之前
      // 所有对共享变量的修改，此处皆可见
      // 此例中，var==77
  });
  // 此处对共享变量 var 修改
  var = 77;
  // 主线程启动子线程
  B.start();
  ```

* **线程 join() 规则**

  这条是关于线程等待的。它是指**主线程 A 等待子线程 B 完成**（主线程 A 通过调用子线程B 的 join() 方法实现），当子线程 B 完成后（主线程 A 中 join() 方法返回），**主线程能够看到子线程的操作**。当然所谓的“看到”，指的是对**共享变量的操作**。

  换句话说就是，如果在线程 A 中，调用线程 B 的 join() 并成功返回，那么线程 B 中的任意操作 Happens-Before 于该 join() 操作的返回。具体可参考下面示例代码。

  ```java
  Thread B = new Thread(()->{
      // 此处对共享变量 var 修改
      var = 66;
  });
  // 例如此处对共享变量修改，
  // 则这个修改结果对线程 B 可见
  // 主线程启动子线程
  B.start();
  B.join()
  // 子线程所有对共享变量的修改
  // 在主线程调用 B.join() 之后皆可见（主线程可见子线程）
  // 此例中，var==66
  ```

  

### 被我们忽视的 final

前面我们讲 volatile 为的是禁用缓存以及编译优化，我们再从另外一个方面来看，有没有办法告诉编译器优化得更好一点呢？这个可以有，就是final 关键字。

final 修饰变量时，初衷是告诉编译器：这个变量生而不变，可以可劲儿优化。Java 编译器在 1.5 以前的版本的确优化得很努力，以至于都优化错了。问题类似于上一期提到的利用双重检查方法创建单例，构造函数的错误重排导致线程可能看到 final 变量的值会变化。

当然在 1.5 以后 Java 内存模型对 final 类型变量的重排进行了约束。现在只要我们提供正确构造函数没有“逸出”，就不会出问题了。“逸出”有点抽象，我们还是举个例子吧，在下面例子中，在构造函数里面将 this 赋值给了全局变量 global.obj，这就是“逸出”，线程通过 global.obj 读取 x 是有可能读到 0 的。因此我们一定要避免“逸出”。

```java
// 以下代码来源于【参考 1】
final int x;
// 错误的构造函数
public FinalFieldExample() { 
    x = 3;
    y = 4;
    // 此处就是讲 this 逸出，
    global.obj = this;
}
```



总结：

* 在 Java 语言里面，Happens-Before 的语义本质上是一种可见性，A Happens-Before B 意味着 A 事件对 B 事件来说是可见的，无论 A 事件和 B 事件是否发生在同一个线程里。例如 A 事件发生在线程 1 上，B 事件发生在线程 2 上，Happens-Before 规则保证线程 2上也能看到 A 事件的发生。
* Java 内存模型主要分为两部分，一部分面向你我这种编写并发程序的应用开发人员，另一部分是面向 JVM 的实现人员的，我们可以重点关注前者，也就是和编写并发程序相关的部分，这部分内容的核心就是 Happens-Before 规则。相信经过本章的介绍，你应该对这部分内容已经有了深入的认识。

